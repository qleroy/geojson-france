{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525cd22-6524-411f-98e6-fc8544f4ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import py7zr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import affinity\n",
    "import shapely.geometry\n",
    "import shapely.ops\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "# Dossier de travail -----------------------------------------------------------\n",
    "DATA_DIR = Path(\"./data\").expanduser()\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Style de tracé par défaut pour GeoDataFrame.plot()\n",
    "PLOT_STYLES = {\n",
    "    \"edgecolor\": \"black\",  # couleur des contours\n",
    "    \"column\": \"name\",      # colonne utilisée pour la coloration catégorielle\n",
    "    \"legend\": False,       # pas de légende\n",
    "    \"cmap\": \"tab20\",       # palette discrète\n",
    "    \"linewidth\": 0.25,     # épaisseur des contours\n",
    "}\n",
    "\n",
    "\n",
    "# Utilitaires E/S --------------------------------------------------------------\n",
    "def download_file(url: str, dest_path: Path, chunk_size: int = 8192) -> Path:\n",
    "    \"\"\"\n",
    "    Télécharge un fichier en streaming vers dest_path.\n",
    "\n",
    "    Args:\n",
    "        url: URL du fichier à télécharger.\n",
    "        dest_path: chemin local de sortie (répertoires créés si besoin).\n",
    "        chunk_size: taille des blocs d'écriture.\n",
    "\n",
    "    Returns:\n",
    "        Le chemin du fichier téléchargé.\n",
    "    \"\"\"\n",
    "    dest_path = Path(dest_path)\n",
    "    dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with requests.get(url, stream=True) as resp:\n",
    "        resp.raise_for_status()\n",
    "        with dest_path.open(\"wb\") as f:\n",
    "            for chunk in resp.iter_content(chunk_size=chunk_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    return dest_path\n",
    "\n",
    "\n",
    "def unzip_7z(archive_path: Path, extract_dir: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Décompresse une archive .7z dans un dossier cible.\n",
    "\n",
    "    Args:\n",
    "        archive_path: chemin de l'archive .7z\n",
    "        extract_dir: dossier de destination\n",
    "\n",
    "    Returns:\n",
    "        Le dossier d'extraction.\n",
    "    \"\"\"\n",
    "    archive_path = Path(archive_path)\n",
    "    extract_dir = Path(extract_dir)\n",
    "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with py7zr.SevenZipFile(archive_path, mode=\"r\") as z:\n",
    "        z.extractall(path=extract_dir)\n",
    "\n",
    "    return extract_dir\n",
    "\n",
    "\n",
    "def convert_shp_to_geojson(shapefile_path: Path, geojson_path: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Charge un shapefile et l’exporte en GeoJSON.\n",
    "\n",
    "    Args:\n",
    "        shapefile_path: chemin vers le .shp\n",
    "        geojson_path: chemin de sortie .geojson\n",
    "\n",
    "    Returns:\n",
    "        Le chemin du GeoJSON écrit.\n",
    "    \"\"\"\n",
    "    shapefile_path = Path(shapefile_path)\n",
    "    geojson_path = Path(geojson_path)\n",
    "    geojson_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    gdf.to_file(geojson_path, driver=\"GeoJSON\")\n",
    "    return geojson_path\n",
    "\n",
    "\n",
    "# Géométrie / carto ------------------------------------------------------------\n",
    "def reposition(\n",
    "    gdf: gpd.GeoDataFrame,\n",
    "    idx,  # index, liste d'index ou masque booléen sélectionnant les entités à déplacer\n",
    "    xoff: Optional[float] = None,\n",
    "    yoff: Optional[float] = None,\n",
    "    xscale: Optional[float] = None,\n",
    "    yscale: Optional[float] = None,\n",
    "    simplify: Optional[float] = None,\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Mise à l’échelle et translation d’un sous-ensemble de géométries autour de leur centroïde commun.\n",
    "\n",
    "    - Le centroïde d'origine est calculé sur l’union des géométries sélectionnées.\n",
    "    - L’échelle est appliquée avant la translation.\n",
    "    - La simplification (si fournie) intervient à la fin (preserve_topology=False).\n",
    "\n",
    "    Args:\n",
    "        gdf: GeoDataFrame source.\n",
    "        idx: indices/masque pour sélectionner les lignes à transformer.\n",
    "        xoff, yoff: décalages (en unités du CRS) à appliquer.\n",
    "        xscale, yscale: facteurs d’échelle (1 = inchangé).\n",
    "        simplify: tolérance pour la simplification (None = pas de simplification).\n",
    "\n",
    "    Returns:\n",
    "        Un nouveau GeoDataFrame avec les géométries modifiées.\n",
    "    \"\"\"\n",
    "    # union des géométries sélectionnées pour obtenir un point d’ancrage robuste\n",
    "    anchor = gdf.loc[idx, \"geometry\"].union_all().centroid\n",
    "\n",
    "    def _transform(geom):\n",
    "        out = geom\n",
    "        if xscale is not None or yscale is not None:\n",
    "            out = affinity.scale(out, xfact=xscale or 1, yfact=yscale or 1, origin=anchor)\n",
    "        if xoff is not None or yoff is not None:\n",
    "            out = affinity.translate(out, xoff or 0, yoff or 0)\n",
    "        if simplify is not None and simplify > 0:\n",
    "            out = out.simplify(simplify, preserve_topology=False)\n",
    "        return out\n",
    "\n",
    "    res = gdf.copy()\n",
    "    res.loc[idx, \"geometry\"] = res.loc[idx, \"geometry\"].apply(_transform)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a149fc13-5dcb-46ca-8073-a156b8467deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# Téléchargement et préparation des données ADMIN-EXPRESS (IGN)\n",
    "# Source : https://geoservices.ign.fr/adminexpress\n",
    "# ---\n",
    "\n",
    "# URL du fichier compressé (.7z) à télécharger\n",
    "adminexpress_url = (\n",
    "    \"https://data.geopf.fr/telechargement/download/ADMIN-EXPRESS/\"\n",
    "    \"ADMIN-EXPRESS_3-2__SHP_WGS84G_FRA_2025-02-17/\"\n",
    "    \"ADMIN-EXPRESS_3-2__SHP_WGS84G_FRA_2025-02-17.7z\"\n",
    ")\n",
    "\n",
    "# Chemins locaux pour le fichier compressé et pour l’extraction\n",
    "archive_path = DATA_DIR / \"ADMIN-EXPRESS_3-2__SHP_WGS84G_FRA_2025-02-17.7z\"\n",
    "extraction_dir = DATA_DIR / \"ADMIN-EXPRESS_3-2__SHP_WGS84G_FRA_2025-02-17\"\n",
    "\n",
    "# Étape 1 : Téléchargement du fichier ADMIN-EXPRESS\n",
    "download_file(adminexpress_url, archive_path)\n",
    "print(\"✅ Téléchargement terminé :\", archive_path)\n",
    "\n",
    "# Étape 2 : Décompression de l’archive\n",
    "unzip_7z(archive_path, DATA_DIR)\n",
    "print(\"✅ Extraction terminée :\", extraction_dir)\n",
    "\n",
    "# Répertoire contenant les shapefiles après extraction\n",
    "shapefile_dir = (\n",
    "    extraction_dir / \"ADMIN-EXPRESS\" /\n",
    "    \"1_DONNEES_LIVRAISON_2025-02-00188\" /\n",
    "    \"ADE_3-2_SHP_WGS84G_FRA-ED2025-02-17\"\n",
    ")\n",
    "\n",
    "# Chemin de sortie pour le GeoJSON\n",
    "commune_geojson_path = DATA_DIR / \"COMMUNE.geojson\"\n",
    "\n",
    "# Étape 3 : Conversion du shapefile en GeoJSON\n",
    "convert_shp_to_geojson(shapefile_dir / \"COMMUNE.shp\", commune_geojson_path)\n",
    "print(\"✅ Conversion terminée : COMMUNE.shp → COMMUNE.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6613c1-9667-4ee4-a49b-52fc33236506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# Chargement et simplification du fichier COMMUNE.geojson\n",
    "# ---\n",
    "\n",
    "# Étape 1 : Chargement du GeoJSON issu d'ADMIN-EXPRESS\n",
    "communes_admin_express = gpd.read_file(DATA_DIR / \"COMMUNE.geojson\")\n",
    "print(\"✅ Fichier COMMUNE.geojson chargé depuis ADMIN-EXPRESS\")\n",
    "\n",
    "# Étape 2 : Sélection des colonnes utiles et sauvegarde simplifiée\n",
    "def save_as_geojson(gdf, keep_columns, output_filename):\n",
    "    \"\"\"\n",
    "    Sélectionne certaines colonnes d’un GeoDataFrame et les exporte en GeoJSON.\n",
    "\n",
    "    Args:\n",
    "        gdf: GeoDataFrame source.\n",
    "        keep_columns: liste des colonnes à conserver (y compris 'geometry').\n",
    "        output_filename: nom du fichier de sortie (dans DATA_DIR).\n",
    "    \"\"\"\n",
    "    simplified = gdf[keep_columns].copy()\n",
    "    output_path = DATA_DIR / output_filename\n",
    "    simplified.to_file(output_path, driver=\"GeoJSON\")\n",
    "    print(f\"✅ GeoJSON simplifié sauvegardé : {output_path}\")\n",
    "\n",
    "# Exemple d’utilisation :\n",
    "columns_to_keep = [\"NOM\", \"INSEE_COM\", \"INSEE_DEP\", \"INSEE_REG\", \"geometry\"]\n",
    "save_as_geojson(communes_admin_express, columns_to_keep, \"communes.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc5996-8464-4f03-a116-55e824058661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# Téléchargement et chargement des fichiers COG 2025 (INSEE)\n",
    "# Source : https://www.insee.fr/fr/information/8377162\n",
    "# ---\n",
    "\n",
    "# Étape 1 : Téléchargement des fichiers CSV (communes, départements, régions)\n",
    "download_file(\n",
    "    \"https://www.insee.fr/fr/statistiques/fichier/8377162/v_commune_2025.csv\",\n",
    "    DATA_DIR / \"v_commune_2025.csv\",\n",
    ")\n",
    "download_file(\n",
    "    \"https://www.insee.fr/fr/statistiques/fichier/8377162/v_departement_2025.csv\",\n",
    "    DATA_DIR / \"v_departement_2025.csv\",\n",
    ")\n",
    "download_file(\n",
    "    \"https://www.insee.fr/fr/statistiques/fichier/8377162/v_region_2025.csv\",\n",
    "    DATA_DIR / \"v_region_2025.csv\",\n",
    ")\n",
    "print(\"✅ Fichiers COG 2025 téléchargés\")\n",
    "\n",
    "# Étape 2 : Chargement des données\n",
    "# Communes\n",
    "cog_communes = pd.read_csv(DATA_DIR / \"v_commune_2025.csv\")\n",
    "print(\"✅ v_commune_2025.csv chargé :\", cog_communes.shape)\n",
    "\n",
    "# Départements (on garde uniquement le code et le libellé)\n",
    "cog_departements = pd.read_csv(DATA_DIR / \"v_departement_2025.csv\")\n",
    "cog_departements = cog_departements[[\"DEP\", \"LIBELLE\"]]\n",
    "print(\"✅ v_departement_2025.csv chargé :\", cog_departements.shape)\n",
    "\n",
    "# Régions (on garde uniquement le code et le libellé)\n",
    "cog_regions = pd.read_csv(DATA_DIR / \"v_region_2025.csv\")\n",
    "cog_regions = cog_regions[[\"REG\", \"LIBELLE\"]]\n",
    "print(\"✅ v_region_2025.csv chargé :\", cog_regions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a91182-d059-4be3-ae38-3636a1d9913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# Compétence territoriale de la Police nationale et de la Gendarmerie\n",
    "# Source : https://www.data.gouv.fr/fr/datasets/competence-territoriale-gendarmerie-et-police-nationales/\n",
    "# ---\n",
    "\n",
    "# Étape 1 : Téléchargement du fichier CSV\n",
    "download_file(\n",
    "    \"https://www.data.gouv.fr/fr/datasets/r/c53cd4d4-4623-4772-9b8c-bc72a9cdf4c2\",\n",
    "    DATA_DIR / \"competences_pn_gn.csv\",\n",
    ")\n",
    "print(\"✅ Fichier competences_pn_gn.csv téléchargé\")\n",
    "\n",
    "# Étape 2 : Chargement avec séparateur « ; »\n",
    "competences_pn_gn = pd.read_csv(DATA_DIR / \"competences_pn_gn.csv\", sep=\";\")\n",
    "print(\"✅ competences_pn_gn.csv chargé :\", competences_pn_gn.shape)\n",
    "\n",
    "# Étape 3 : Filtrage des communes relevant de la Police nationale\n",
    "communes_pn = competences_pn_gn[competences_pn_gn[\"institution\"] == \"PN\"]\n",
    "print(\"✅ Communes sous compétence Police nationale :\", communes_pn.shape)\n",
    "\n",
    "# Étape 4 : Sauvegarde du sous-ensemble\n",
    "output_path = DATA_DIR / \"communes_pn.csv\"\n",
    "communes_pn.to_csv(output_path, index=False)\n",
    "print(\"✅ Fichier sauvegardé :\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d77d3a1-382a-48c4-85d3-377cd9aeb10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# Enrichissement ADMIN-EXPRESS avec COG + compétence PN (CPN),\n",
    "# repositionnement des DROM, puis agrégation par service CPN\n",
    "# ---\n",
    "\n",
    "# 1) Chargement des jeux\n",
    "communes_ae = gpd.read_file(DATA_DIR / \"COMMUNE.geojson\")\n",
    "print(\"✅ ADMIN-EXPRESS chargé :\", communes_ae.shape)\n",
    "\n",
    "cpn = pd.read_csv(DATA_DIR / \"communes_pn.csv\")   # si tu as suivi le renommage précédent -> \"communes_pn.csv\"\n",
    "print(\"✅ communes_pn.csv chargé :\", cpn.shape)\n",
    "\n",
    "# 2) Normalisation des noms de colonnes\n",
    "communes_ae = communes_ae.rename(columns={\"NOM\": \"name_commune\"})\n",
    "\n",
    "# 3) Jointures avec le COG (communes, départements, régions)\n",
    "#    - On utilise des suffixes explicites pour éviter les collisions de 'LIBELLE'\n",
    "communes_ae = pd.merge(\n",
    "    communes_ae,\n",
    "    cog_communes,\n",
    "    left_on=\"INSEE_COM\",\n",
    "    right_on=\"COM\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_cogcom\"),\n",
    ")\n",
    "print(\"✅ Fusion avec COG communes :\", communes_ae.shape)\n",
    "\n",
    "communes_ae = pd.merge(\n",
    "    communes_ae,\n",
    "    cog_departements.rename(columns={\"LIBELLE\": \"name_dep\"}),\n",
    "    on=\"DEP\",\n",
    "    how=\"left\",\n",
    ")\n",
    "print(\"✅ Fusion avec COG départements :\", communes_ae.shape)\n",
    "\n",
    "# 4) Repositionnement visuel des Outre-mer (mise à l’échelle + translation)\n",
    "communes_ae = reposition(communes_ae, communes_ae.name_dep == \"Guadeloupe\",  57.4, 25.4, 1.5, 1.5)\n",
    "communes_ae = reposition(communes_ae, communes_ae.name_dep == \"Martinique\",  58.4, 27.1, 1.5, 1.5)\n",
    "communes_ae = reposition(communes_ae, communes_ae.name_dep == \"Guyane\",      52.0, 37.7, 0.35, 0.35)\n",
    "communes_ae = reposition(communes_ae, communes_ae.name_dep == \"La Réunion\", -55.0, 62.8, 1.5, 1.5)\n",
    "communes_ae = reposition(communes_ae, communes_ae.name_dep == \"Mayotte\",    -43.0, 54.3, 1.5, 1.5)\n",
    "print(\"✅ Repositionnement des Outre-mer terminé\")\n",
    "\n",
    "# 5) Jointure avec COG régions + libellé\n",
    "communes_ae = pd.merge(\n",
    "    communes_ae,\n",
    "    cog_regions.rename(columns={\"LIBELLE\": \"name_reg\"}),\n",
    "    on=\"REG\",\n",
    "    how=\"left\",\n",
    ")\n",
    "print(\"✅ Fusion avec COG régions :\", communes_ae.shape)\n",
    "\n",
    "# 6) Ajout de la zone de défense (mapping sûr via dict.get)\n",
    "map_region_to_dz = {\n",
    "    \"Auvergne-Rhône-Alpes\": \"Sud-Est\",\n",
    "    \"Bourgogne-Franche-Comté\": \"Est\",\n",
    "    \"Bretagne\": \"Ouest\",\n",
    "    \"Centre-Val de Loire\": \"Ouest\",\n",
    "    \"Corse\": \"Sud\",\n",
    "    \"Grand Est\": \"Est\",\n",
    "    \"Guadeloupe\": \"Antilles\",\n",
    "    \"Guyane\": \"Guyane\",\n",
    "    \"Hauts-de-France\": \"Nord\",\n",
    "    \"La Réunion\": \"Sud de l'Océan Indien\",\n",
    "    \"Martinique\": \"Antilles\",\n",
    "    \"Mayotte\": \"Sud de l'Océan Indien\",\n",
    "    \"Normandie\": \"Ouest\",\n",
    "    \"Nouvelle-Aquitaine\": \"Sud-Ouest\",\n",
    "    \"Occitanie\": \"Sud\",\n",
    "    \"Pays de la Loire\": \"Ouest\",\n",
    "    \"Provence-Alpes-Côte d'Azur\": \"Sud\",\n",
    "    \"Île-de-France\": \"Paris\",\n",
    "}\n",
    "communes_ae[\"name_dz\"] = communes_ae[\"name_reg\"].apply(lambda x: map_region_to_dz.get(x, \"Inconnue\"))\n",
    "\n",
    "# 7) Garde seulement les communes relevant de la PN (CPN)\n",
    "#    - aligne les types des codes si besoin\n",
    "comm_insee = communes_ae[\"INSEE_COM\"].astype(str)\n",
    "codes_cpn = cpn[\"code_commune\"].astype(str)\n",
    "mask_pn = comm_insee.isin(codes_cpn)\n",
    "\n",
    "communes_pn_ae = communes_ae.loc[mask_pn].copy()\n",
    "print(\"✅ Communes sous compétence PN retenues :\", communes_pn_ae.shape)\n",
    "\n",
    "# 8) Jointure avec le fichier CPN pour récupérer le nom du service\n",
    "communes_pn_ae = pd.merge(\n",
    "    communes_pn_ae,\n",
    "    cpn[[\"code_commune\", \"service\"]],\n",
    "    left_on=\"INSEE_COM\",\n",
    "    right_on=\"code_commune\",\n",
    "    how=\"left\",\n",
    ")\n",
    "cols_communes_pn = [\"REG\", \"DEP\", \"name_dep\", \"name_reg\", \"name_dz\", \"service\", \"name_commune\", \"geometry\"]\n",
    "save_as_geojson(communes_pn_ae, cols_communes_pn, \"communes_pn.geojson\")\n",
    "\n",
    "cpn_ae = communes_pn_ae.dissolve(by=\"service\").reset_index()\n",
    "\n",
    "print(\"✅ Agrégation par service réalisée :\", cpn_ae.shape)\n",
    "\n",
    "# 9) Contrôle éventuel : lignes sans DEP (après agrégation)\n",
    "if \"DEP\" in cpn_ae.columns:\n",
    "    print(\"ℹ️ Services avec DEP manquant :\")\n",
    "    print(cpn_ae[cpn_ae[\"DEP\"].isna()][[\"service\"]])\n",
    "\n",
    "# 10) Export final des CPN\n",
    "cols_cpn = [\"REG\", \"DEP\", \"name_dep\", \"name_reg\", \"name_dz\", \"service\", \"geometry\"]\n",
    "save_as_geojson(cpn_ae, cols_cpn, \"cpn.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef34f736-5139-4fe7-9c89-a21c42d70b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# Construction des zones de défense à partir des communes ADMIN-EXPRESS enrichies\n",
    "# ---\n",
    "\n",
    "# Agrégation des communes par zone de défense\n",
    "# dz_ae = communes_ae.dissolve(by=\"name_dz\").reset_index()\n",
    "# Pourquoi un dissolve direct par zone de défense depuis les communes pose problème :\n",
    "# - Topologie hétérogène à l’échelle communale : de micro-chevauchements, interstices et\n",
    "#   géométries multipart (îlots, polygones littoraux) existent entre communes. Un dissolve\n",
    "#   \"global\" sur des milliers de petites frontières cumule ces défauts, laissant des trous\n",
    "#   ou créant des lignes internes résiduelles dans la zone de défense finale.\n",
    "# - Limites non parfaitement coïncidentes : certaines communes côtières/rivières et\n",
    "#   communes déléguées ont des découpages particuliers (zones maritimes, bancs, enclaves).\n",
    "#   Dissoudre directement à l’échelle DZ agrège des frontières qui ne s’imbriquent pas\n",
    "#   exactement et dégrade la topologie.\n",
    "# - Charge et robustesse : fusionner d’un coup un très grand nombre de polygones est coûteux\n",
    "#   et plus susceptible d’échouer (erreurs GEOS, self-intersections) ou de produire des\n",
    "#   géométries invalides.\n",
    "# - Perte de l’alignement administratif : la frontière « officielle » d’une DZ est la somme\n",
    "#   des régions (elles-mêmes somme des départements, etc.). Dissoudre par paliers\n",
    "#   communes ➜ départements ➜ régions ➜ zones de défense réduit le nombre d’arêtes à\n",
    "#   fusionner à chaque étape, corrige implicitement beaucoup de micro-écarts, et garantit\n",
    "#   que chaque niveau hérite de limites cohérentes et propres.\n",
    "# => En pratique, le dissolve progressif est plus stable, plus rapide et produit des\n",
    "#    géométries valides et propres, conformes aux niveaux administratifs intermédiaires.\n",
    "dep_ae = communes_ae.dissolve(by=\"name_dep\").reset_index()\n",
    "reg_ae = dep_ae.dissolve(by=\"name_reg\").reset_index()\n",
    "dz_ae = reg_ae.dissolve(by=\"name_dz\").reset_index()\n",
    "\n",
    "\n",
    "print(\"✅ Agrégation par zones de défense terminée :\", dz_ae.shape)\n",
    "\n",
    "# Colonnes à garder\n",
    "dz_keep_cols = [\"name_dz\", \"geometry\"]\n",
    "\n",
    "# Sauvegarde en GeoJSON\n",
    "save_as_geojson(dz_ae, dz_keep_cols, \"dz.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b4c730-c46b-4691-8b5b-35f38a21ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export des zones DZ en GeoJSON\n",
    "for dz_name in cpn_ae['name_dz'].unique():\n",
    "    dz_subset = cpn_ae[cpn_ae['name_dz'] == dz_name]\n",
    "    save_as_geojson(dz_subset, dz_keep_cols, f\"dz_{dz_name}.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e95da76-f933-4e6e-a029-03b0bd309946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde de la correspondance départements (code INSEE ↔ nom)\n",
    "departements_ref = (\n",
    "    communes_ae[['INSEE_DEP', 'name_dep']]\n",
    "    .drop_duplicates()\n",
    "    .sort_values('INSEE_DEP')\n",
    ")\n",
    "departements_ref.to_csv(DATA_DIR / 'insee_dep_name_dep.csv', index=False)\n",
    "\n",
    "# Sauvegarde de la correspondance régions (code INSEE ↔ nom)\n",
    "regions_ref = (\n",
    "    communes_ae[['INSEE_REG', 'name_reg']]\n",
    "    .drop_duplicates()\n",
    "    .sort_values('INSEE_REG')\n",
    ")\n",
    "regions_ref.to_csv(DATA_DIR / 'insee_reg_name_reg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ac241a-6bed-4fa4-a099-25206f390081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
